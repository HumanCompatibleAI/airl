"""Behavioural Cloning (BC).

Trains policy by applying supervised learning to a fixed dataset of (observation,
action) pairs generated by some expert demonstrator.
"""

import collections
from typing import Any, Callable, Dict, List, Mapping, Optional, Tuple, Type, Union

import gym
import numpy as np
import torch as th
from stable_baselines3.common import logger
from stable_baselines3.common.policies import ActorCriticPolicy
from stable_baselines3.common.utils import get_schedule_fn, update_learning_rate
from tqdm.autonotebook import trange

from imitation.data import dataset, types
from imitation.policies.base import FeedForward32Policy


class BC:
    def __init__(
        self,
        observation_space: gym.Space,
        action_space: gym.Space,
        *,
        policy_class: Type[ActorCriticPolicy] = FeedForward32Policy,
        policy_kwargs: Optional[Mapping[str, Any]] = None,
        expert_data: Union[
            types.Transitions, dataset.Dataset[Dict[str, np.ndarray]], None
        ] = None,
        batch_size: int = 32,
        optimizer_cls: Type[th.optim.Optimizer] = th.optim.Adam,
        optimizer_kwargs: Optional[Dict[str, Any]] = None,
        ent_weight: float = 1e-3,
        l2_weight: float = 0.0,
    ):
        """Behavioral cloning (BC).

        Recovers a policy via supervised learning on a Dataset of observation-action
        pairs.

        This class borrows code from Stable Baselines 3
        (https://github.com/DLR-RM/stable-baselines3/).

        Args:
            observation_space: the observation space of the environment.
            action_space: the action space of the environment.
            policy_class: used to instantiate imitation policy.
            policy_kwargs: keyword arguments passed to policy's constructor.
            expert_data: If not None, then immediately call
                  `self.set_expert_dataset(expert_data)` during initialization.
            batch_size: batch size used for training.
            optimizer_cls: optimiser to use for supervised training.
            optimizer_kwargs: keyword arguments, excluding learning rate and
                  weight decay, for optimiser construction.
            ent_weight: scaling applied to the policy's entropy regularization.
            l2_weight: scaling applied to the policy's L2 regularization.
        """
        if optimizer_kwargs:
            if "lr" in optimizer_kwargs:
                raise ValueError(
                    "The policy's lr_schedule determines the learning rate."
                )
            if "weight_decay" in optimizer_kwargs:
                raise ValueError(
                    "Use the parameter l2_weight to specify the weight decay."
                )
        if l2_weight != 0.0:
            optimizer_kwargs = collections.ChainMap(
                dict(weight_decay=l2_weight), optimizer_kwargs or {}
            )

        self.action_space = action_space
        self.observation_space = observation_space
        self.policy_class = policy_class
        policy_kwargs_defaults = dict(
            observation_space=self.observation_space,
            action_space=self.action_space,
            lr_schedule=get_schedule_fn(1e-3),
        )
        self.policy_kwargs = collections.ChainMap(
            policy_kwargs or {}, policy_kwargs_defaults
        )
        self.lr_schedule = self.policy_kwargs["lr_schedule"]

        wrapped_optimizer_kwargs = dict(
            optimizer_class=optimizer_cls, optimizer_kwargs=optimizer_kwargs,
        )
        self.policy_kwargs = collections.ChainMap(
            wrapped_optimizer_kwargs, self.policy_kwargs
        )

        self.policy = self.policy_class(
            **self.policy_kwargs
        )  # pytype: disable=not-instantiable

        assert batch_size >= 1
        self.batch_size = batch_size
        self.expert_dataset: Optional[dataset.DictDataset] = None
        self.ent_weight = ent_weight
        self.l2_weight = l2_weight
        # Track the fraction of training remaining (starts from 1 and ends at 0)
        # this is used to update the learning rate
        self._current_progress_remaining = 1.0

        if expert_data is not None:
            self.set_expert_dataset(expert_data)

    def set_expert_dataset(
        self, expert_data: Union[types.Transitions, dataset.DictDataset],
    ) -> None:
        """Replace the current expert dataset with a new one.

        Useful for DAgger and other interactive algorithms.

        Args:
             expert_data: Either a `DictDataset` whose keys include "obs" and "act"
                 and for which `.size()` is not None, or a `Transitions` instance, which
                 is automatically converted to a shuffled `EpochOrderDictDataset`.
        """
        if isinstance(expert_data, types.Transitions):
            trans = expert_data
            expert_dataset = dataset.EpochOrderDictDataset(
                {"obs": trans.obs, "acts": trans.acts}, shuffle=True,
            )
        else:
            expert_dataset = expert_data
        assert expert_dataset.size() is not None
        self.expert_dataset = expert_dataset

    def _update_current_progress_remaining(
        self, num_timesteps: int, total_timesteps: int
    ) -> None:
        """
        Compute current progress remaining (starts from 1 and ends at 0)

        Args:
            num_timesteps: current number of timesteps
            total_timesteps: total number of timesteps
        """
        progress_elapsed = float(num_timesteps) / float(total_timesteps)
        self._current_progress_remaining = 1.0 - progress_elapsed

    def _update_learning_rate(
        self, optimizers: Union[List[th.optim.Optimizer], th.optim.Optimizer],
    ) -> None:
        """
        Update the optimizers' learning rates using the current learning rate schedule
        and the current progress remaining (starts from 1 and ends at 0).

        Args:
            optimizers: An optimizer or a list of optimizers to update.
        """
        if not isinstance(optimizers, list):
            optimizers = [optimizers]
        for optimizer in optimizers:
            update_learning_rate(
                optimizer, self.lr_schedule(self._current_progress_remaining)
            )

    def _calculate_loss(self, obs, acts) -> Tuple[th.Tensor, Dict[str, float]]:
        """
        Calculate the supervised learning loss used to train the behavioral clone.

        Args:
            obs: The observations seen by the expert.
            acts: The actions taken by the expert.

        Returns:
            loss: The supervised learning loss for the behavioral clone to optimize.
            stats_dict: Statistics about the learning process to be logged.

        """
        _, log_prob, entropy = self.policy.evaluate_actions(obs, acts)
        prob_true_act = th.exp(log_prob).mean()
        log_prob = log_prob.mean()
        entropy = entropy.mean()

        ent_loss = -self.ent_weight * entropy
        neglogp = -log_prob
        loss = neglogp + ent_loss

        l2_norms = [th.norm(w, 2) ** 2 for w in self.policy.parameters()]
        l2_norm = sum(l2_norms) / 2  # divide by 2 to cancel with gradient of square
        l2_loss = self.l2_weight * l2_norm

        stats_dict = dict(
            neglogp=neglogp.item(),
            loss=loss.item(),
            entropy=entropy.item(),
            ent_loss=ent_loss.item(),
            prob_true_act=prob_true_act.item(),
            l2_norm=l2_norm.item(),
            l2_loss=l2_loss.item(),
        )

        return loss, stats_dict

    def train(
        self,
        n_epochs: int = 100,
        *,
        on_epoch_end: Callable[[dict], None] = None,
        log_interval: int = 100,
    ):
        """Train with supervised learning for some number of epochs.

        Here an 'epoch' is just a complete pass through the expert transition
        dataset.

        Args:
          n_epochs: number of complete passes made through dataset.
          on_epoch_end: optional callback to run at
            the end of each epoch. Will receive all locals from this function as
            dictionary argument (!!).
          log_interval: log stats after every log_interval batches
        """
        assert self.batch_size >= 1
        total_samples = n_epochs * self.expert_dataset.size()
        total_batches = np.ceil(total_samples / self.batch_size)
        samples_so_far = 0
        batch_num = 0
        for epoch_num in trange(n_epochs, desc="BC epoch"):
            while samples_so_far < (epoch_num + 1) * self.expert_dataset.size():
                self._update_current_progress_remaining(batch_num, total_batches)
                self._update_learning_rate(self.policy.optimizer)
                batch_num += 1
                batch_dict = self.expert_dataset.sample(self.batch_size)
                assert len(batch_dict["obs"]) == self.batch_size
                samples_so_far += self.batch_size

                obs_tensor = th.as_tensor(batch_dict["obs"]).to(self.policy.device)
                acts_tensor = th.as_tensor(batch_dict["acts"]).to(self.policy.device)
                loss, stats_dict = self._calculate_loss(obs_tensor, acts_tensor)

                self.policy.optimizer.zero_grad()
                loss.backward()
                self.policy.optimizer.step()
                stats_dict["epoch_num"] = epoch_num
                stats_dict["n_updates"] = batch_num
                stats_dict["batch_size"] = len(batch_dict["obs"])

                if batch_num % log_interval == 0:
                    for k, v in stats_dict.items():
                        logger.record(k, v)
                    logger.dump()

            if on_epoch_end is not None:
                on_epoch_end(locals())

    def save_policy(self, policy_path: str) -> None:
        """Save policy to a path. Can be reloaded by `.reconstruct_policy()`.

        Args:
            policy_path: path to save policy to.
        """
        self.policy.save(policy_path)

    def load_policy(self, *args, **kwargs) -> None:
        """Load policy that `.save_policy()` previously saved.

        The `args` and `kwargs` are passed to `.reconstruct_policy()`.
        """
        self.policy = self.reconstruct_policy(*args, **kwargs)

    @staticmethod
    def reconstruct_policy(
        policy_path: str, device: Union[th.device, str] = "auto",
    ) -> ActorCriticPolicy:
        """Reconstruct a saved policy.

        Args:
            policy_path: path where `.save_policy()` has been run.
            device: device on which to load the policy.

        Returns:
            policy: policy with reloaded weights.
        """
        return ActorCriticPolicy.load(policy_path, device)
