"""Behavioural Cloning (BC).

Trains policy by applying supervised learning to a fixed dataset of (observation,
action) pairs generated by some expert demonstrator.
"""

import os
from typing import Callable, List, Optional, Type, Union

import cloudpickle
import numpy as np
import tensorflow as tf
from stable_baselines.common.policies import ActorCriticPolicy, BasePolicy
from tqdm.autonotebook import trange

from imitation.data import dataset, rollout, types
from imitation.policies.base import FeedForward32Policy


def set_tf_vars(
    *,
    values: List[np.ndarray],
    scope: Optional[str] = None,
    tf_vars: Optional[List[tf.Variable]] = None,
    sess: Optional[tf.Session] = None,
):
    """Set a collection of variables to take the values in `values`.

    Variables can be either specified by scope or passed directly into the
    function as a list. Variables and values will be matched based on the order
    in which they appear in their respective collections, so there must be as
    many values as variables.

    Args:
        values: list of values to load into variables.
        scope: scope to collect variables from. Either this argument xor
          `tf_vars` must be given.
        tf_vars: explicit list of TF variables to write to. Mutex with `scope`.
        sess: TF session to use, if not the default.
    """
    if scope is not None:
        assert tf_vars is None, "must give either `tf_vars` xor `scope` kwargs"
        tf_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope)
    else:
        assert tf_vars is not None, "must give either `tf_vars` xor `scope` kwargs"
    assert len(tf_vars) == len(values), (
        f"{len(tf_vars)} tf variables but " f"{len(values)} values supplied"
    )
    sess = sess or tf.get_default_session()
    assert sess is not None, "must supply session or have one in context"
    placeholders = [tf.placeholder(shape=v.shape, dtype=v.dtype) for v in tf_vars]
    assign_ops = [tf.assign(var, ph) for var, ph in zip(tf_vars, placeholders)]
    sess.run(
        assign_ops, feed_dict={ph: value for ph, value in zip(placeholders, values)}
    )


class BCTrainer:
    def __init__(
        self,
        env,
        *,
        policy_class: Type[ActorCriticPolicy] = FeedForward32Policy,
        expert_dataset_or_trans: Union[
            types.Transitions, dataset.DictDataset, None
        ] = None,  # noqa: E501
        batch_size: int = 32,
        optimiser_cls: Type[tf.train.Optimizer] = tf.train.AdamOptimizer,
        optimiser_kwargs: Optional[dict] = None,
        name_scope: Optional[str] = None,
        reuse: bool = False,
    ):
        """Simple behavioural cloning (BC).

        Recovers only a policy.

        Args:
          env: environment to train on.
          expert_dataset_or_trans:
          policy_class: used to instantiate imitation policy.
          batch_size: batch size used for training.
          optimiser_cls: optimiser to use for supervised training.
          optimiser_kwargs: keyword arguments to pass to optimiser when constructing
              it.
        """
        self.env = env
        self.policy_class = policy_class
        assert batch_size >= 1
        self.batch_size = batch_size
        self.expert_dataset: Optional[dataset.DictDataset] = None
        if expert_dataset_or_trans is not None:
            self.set_expert_dataset(expert_dataset_or_trans)
        self.sess = tf.get_default_session()
        assert self.sess is not None, "need to construct this within a session scope"
        self._build_tf_graph()
        self.sess.run(tf.global_variables_initializer())

    def set_expert_dataset(
        self, expert_dataset_or_trans: Union[types.Transitions, dataset.DictDataset],
    ):
        """Replace the current expert dataset with a new one.

        Useful for DAgger and other interactive algorithms.

        Args:
             expert_dataset: Either a `DictDataset` whose keys include "obs" and "act"
                 and for which `.size()` is not None, or a
        """
        if isinstance(expert_dataset_or_trans, types.Transitions):
            trans = expert_dataset_or_trans
            expert_dataset = dataset.EpochOrderDictDataset(
                {"obs": trans.obs, "act": trans.acts}, shuffle=True,
            )
        else:
            expert_dataset = expert_dataset_or_trans
        assert expert_dataset.size() is not None
        self.expert_dataset = expert_dataset

    def train(
        self, *, n_epochs: int = 100, on_epoch_end: Callable[[dict], None] = None
    ):
        """Train with supervised learning for some number of epochs.

        Here an 'epoch' is just a complete pass through the expert transition
        dataset.

        Args:
          n_epochs: number of complete passes made through dataset.
          on_epoch_end: optional callback to run at
            the end of each epoch. Will receive all locals from this function as
            dictionary argument (!!).
        """
        assert self.batch_size >= 1
        samples_so_far = 0
        for epoch_num in trange(n_epochs, desc="BC epoch"):
            while samples_so_far < (epoch_num + 1) * self.expert_dataset.size():
                batch_dict = self.expert_dataset.sample(self.batch_size)
                assert len(batch_dict["obs"]) == self.batch_size
                samples_so_far += self.batch_size
                feed_dict = {
                    self._true_acts_ph: batch_dict["act"],
                    self.policy.obs_ph: batch_dict["obs"],
                }
                _, loss = self.sess.run(
                    [self._train_op, self._log_loss], feed_dict=feed_dict
                )
            if on_epoch_end is not None:
                on_epoch_end(locals())

    def test_policy(self, *, min_episodes: int = 10) -> dict:
        """Test current imitation policy on environment & give some rollout stats.

        Args:
          min_episodes: Minimum number of rolled-out episodes.

        Returns:
          rollout statistics collected by `imitation.utils.rollout.rollout_stats()`.
        """
        trajs = rollout.generate_trajectories(
            self.policy, self.env, sample_until=rollout.min_episodes(min_episodes)
        )
        reward_stats = rollout.rollout_stats(trajs)
        return reward_stats

    def _build_tf_graph(self):
        with tf.variable_scope("bc_supervised_loss"):
            with tf.variable_scope("model"):
                self.policy_kwargs = dict(
                    ob_space=self.env.observation_space,
                    ac_space=self.env.action_space,
                    n_batch=None,
                    n_env=1,
                    n_steps=1000,
                )
                self.policy = self.policy_class(
                    sess=self.sess, **self.policy_kwargs
                )  # pytype: disable=not-instantiable
                inner_scope = tf.get_variable_scope().name
                self.policy_variables = tf.get_collection(
                    tf.GraphKeys.TRAINABLE_VARIABLES, scope=inner_scope
                )
            self._true_acts_ph = self.policy.pdtype.sample_placeholder(
                [None], name="ref_acts_ph"
            )
            self._log_loss = tf.reduce_mean(
                self.policy.proba_distribution.neglogp(self._true_acts_ph)
            )
            # FIXME: it should be possible to customise both optimiser class and
            # optimiser arguments
            opt = tf.train.AdamOptimizer()
            self._train_op = opt.minimize(self._log_loss)

    def save_policy(self, policy_path: str):
        """Save a policy to a pickle. Can be reloaded by `.reconstruct_policy()`.

        Args:
            policy_path: path to save policy to.
        """
        policy_params = self.sess.run(self.policy_variables)
        data = {
            "class": self.policy_class,
            "kwargs": self.policy_kwargs,
            "params": policy_params,
        }
        dirname = os.path.dirname(policy_path)
        if dirname:
            os.makedirs(dirname, exist_ok=True)
        with open(policy_path, "wb") as fp:
            cloudpickle.dump(data, fp)

    @staticmethod
    def reconstruct_policy(
        policy_path: str, sess: Optional[tf.Session] = None,
    ) -> BasePolicy:
        """Reconstruct a saved policy.

        Args:
            policy_path: path a policy produced by `.save_policy()`.
            sess: optional session to construct policy under,
              if not the default session.

        Returns:
            policy: policy with reloaded weights.
        """
        if sess is None:
            sess = tf.get_default_session()
            assert sess is not None, "must supply session via kwarg or context mgr"

        # re-read data from dict
        with open(policy_path, "rb") as fp:
            loaded_pickle = cloudpickle.load(fp)

        # construct the policy class
        klass = loaded_pickle["class"]
        kwargs = loaded_pickle["kwargs"]
        with tf.variable_scope("reconstructed_policy"):
            rv_pol = klass(sess=sess, **kwargs)
            inner_scope = tf.get_variable_scope().name

        # set values for the new policy's parameters
        param_values = loaded_pickle["params"]
        set_tf_vars(values=param_values, scope=inner_scope, sess=sess)

        return rv_pol
